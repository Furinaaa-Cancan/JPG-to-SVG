"""
å§¿åŠ¿ç¼–è¾‘å™¨
æµç¨‹ï¼šåŸå›¾ + æ–°å§¿åŠ¿ â†’ ControlNet â†’ æ–°å›¾ â†’ çŸ¢é‡åŒ–
"""

import torch
import numpy as np
from PIL import Image, ImageDraw
from pathlib import Path
from datetime import datetime
from diffusers import StableDiffusionXLControlNetPipeline, ControlNetModel
from diffusers.utils import load_image
import cv2


class PoseEditor:
    """å§¿åŠ¿ç¼–è¾‘å™¨"""
    
    def __init__(self, device="mps"):
        self.device = device
        self.pipe = None
        
    def load_models(self):
        """åŠ è½½ControlNetå’ŒSDXL"""
        print("Loading ControlNet OpenPose + SDXL...")
        
        # åŠ è½½ControlNet
        controlnet = ControlNetModel.from_pretrained(
            "thibaud/controlnet-openpose-sdxl-1.0",
            torch_dtype=torch.float32
        )
        
        # åŠ è½½SDXL + ControlNet
        self.pipe = StableDiffusionXLControlNetPipeline.from_pretrained(
            "stabilityai/stable-diffusion-xl-base-1.0",
            controlnet=controlnet,
            torch_dtype=torch.float32
        ).to(self.device)
        
        print("âœ… Models loaded!")
    
    def create_pose_image(self, keypoints: dict, size=(1024, 1024)) -> Image.Image:
        """
        ä»å…³é”®ç‚¹åˆ›å»ºå§¿åŠ¿å›¾åƒ
        keypoints: å­—å…¸ï¼ŒåŒ…å«èº«ä½“å„éƒ¨ä½åæ ‡
        """
        img = Image.new('RGB', size, 'black')
        draw = ImageDraw.Draw(img)
        
        # éª¨æ¶è¿æ¥
        connections = [
            ('nose', 'neck'),
            ('neck', 'right_shoulder'), ('neck', 'left_shoulder'),
            ('right_shoulder', 'right_elbow'), ('right_elbow', 'right_wrist'),
            ('left_shoulder', 'left_elbow'), ('left_elbow', 'left_wrist'),
            ('neck', 'right_hip'), ('neck', 'left_hip'),
            ('right_hip', 'right_knee'), ('right_knee', 'right_ankle'),
            ('left_hip', 'left_knee'), ('left_knee', 'left_ankle'),
        ]
        
        # ç»˜åˆ¶éª¨æ¶
        for start, end in connections:
            if start in keypoints and end in keypoints:
                p1 = keypoints[start]
                p2 = keypoints[end]
                if p1 and p2:
                    draw.line([p1, p2], fill='white', width=8)
        
        # ç»˜åˆ¶å…³é”®ç‚¹
        for name, pos in keypoints.items():
            if pos:
                x, y = pos
                r = 10
                draw.ellipse([x-r, y-r, x+r, y+r], fill='red')
        
        return img
    
    def extract_pose_from_image(self, img_path: str) -> Image.Image:
        """ä»å›¾åƒæå–å§¿åŠ¿ï¼ˆç®€åŒ–ç‰ˆï¼šä½¿ç”¨è¾¹ç¼˜æ£€æµ‹æ¨¡æ‹Ÿï¼‰"""
        img = Image.open(img_path).convert('RGB')
        img_np = np.array(img)
        
        # ä½¿ç”¨Cannyè¾¹ç¼˜æ£€æµ‹ä½œä¸ºç®€åŒ–çš„pose
        gray = cv2.cvtColor(img_np, cv2.COLOR_RGB2GRAY)
        edges = cv2.Canny(gray, 100, 200)
        
        # è½¬ä¸ºRGB
        pose_img = Image.fromarray(cv2.cvtColor(edges, cv2.COLOR_GRAY2RGB))
        
        return pose_img
    
    def edit_pose(
        self,
        original_image: str,
        pose_image: str = None,
        target_pose: dict = None,
        prompt: str = "a woman in elegant pose, high quality, detailed",
        negative_prompt: str = "blurry, distorted, ugly, deformed",
        strength: float = 0.8,
        guidance_scale: float = 7.5,
        controlnet_scale: float = 0.8,
        num_inference_steps: int = 30,
        output_dir: str = "output"
    ):
        """
        ç¼–è¾‘å§¿åŠ¿
        
        Args:
            original_image: åŸå§‹å›¾åƒè·¯å¾„
            pose_image: ç›®æ ‡å§¿åŠ¿å›¾åƒè·¯å¾„ï¼ˆå¯é€‰ï¼‰
            target_pose: ç›®æ ‡å§¿åŠ¿å…³é”®ç‚¹ï¼ˆå¯é€‰ï¼‰
            prompt: ç”Ÿæˆæç¤ºè¯
            strength: å˜åŒ–å¼ºåº¦
            controlnet_scale: ControlNetå¼ºåº¦
        """
        output_path = Path(output_dir)
        output_path.mkdir(parents=True, exist_ok=True)
        
        print(f"\n{'='*60}")
        print(f"ğŸ­ Pose Editor")
        print(f"{'='*60}")
        print(f"Input: {original_image}")
        print(f"Prompt: {prompt}")
        print(f"{'='*60}\n")
        
        # åŠ è½½åŸå›¾
        orig_img = load_image(original_image)
        orig_img = orig_img.resize((1024, 1024))
        
        # è·å–å§¿åŠ¿å›¾åƒ
        if pose_image:
            pose_img = load_image(pose_image).resize((1024, 1024))
        elif target_pose:
            pose_img = self.create_pose_image(target_pose, (1024, 1024))
        else:
            # ä»åŸå›¾æå–å§¿åŠ¿
            pose_img = self.extract_pose_from_image(original_image)
            pose_img = pose_img.resize((1024, 1024))
        
        # ä¿å­˜å§¿åŠ¿å›¾
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        pose_path = output_path / f"pose_{timestamp}.png"
        pose_img.save(pose_path)
        print(f"ğŸ“ Pose image saved: {pose_path}")
        
        # ç”Ÿæˆæ–°å›¾åƒ
        print("ğŸ”„ Generating new pose...")
        
        result = self.pipe(
            prompt=prompt,
            negative_prompt=negative_prompt,
            image=pose_img,
            controlnet_conditioning_scale=controlnet_scale,
            guidance_scale=guidance_scale,
            num_inference_steps=num_inference_steps,
        ).images[0]
        
        # ä¿å­˜ç»“æœ
        result_path = output_path / f"new_pose_{timestamp}.png"
        result.save(result_path)
        print(f"âœ… Result saved: {result_path}")
        
        # æ‰“å¼€ç»“æœ
        import subprocess
        subprocess.run(["open", str(result_path)])
        
        return result_path


def main():
    """ç¤ºä¾‹ï¼šæ‰‹åŠ¨æŒ‡å®šæ–°å§¿åŠ¿"""
    
    device = "mps" if torch.backends.mps.is_available() else "cuda" if torch.cuda.is_available() else "cpu"
    print(f"Device: {device}")
    
    editor = PoseEditor(device=device)
    editor.load_models()
    
    # åŸå›¾
    original = "/Volumes/Seagate/SAM3/01_è¾“å…¥å›¾ç‰‡/Ladygaga_2.jpg"
    
    # å®šä¹‰æ–°å§¿åŠ¿ï¼ˆåæ ‡ä¸º1024x1024å›¾åƒä¸Šçš„åƒç´ ä½ç½®ï¼‰
    # ç¤ºä¾‹ï¼šåŒæ‰‹ä¸¾èµ·çš„å§¿åŠ¿
    new_pose = {
        'nose': (512, 200),
        'neck': (512, 280),
        'right_shoulder': (400, 320),
        'left_shoulder': (624, 320),
        'right_elbow': (320, 200),  # ä¸¾èµ·
        'left_elbow': (704, 200),   # ä¸¾èµ·
        'right_wrist': (280, 100),  # é«˜ä¸¾
        'left_wrist': (744, 100),   # é«˜ä¸¾
        'right_hip': (450, 550),
        'left_hip': (574, 550),
        'right_knee': (430, 750),
        'left_knee': (594, 750),
        'right_ankle': (420, 950),
        'left_ankle': (604, 950),
    }
    
    editor.edit_pose(
        original_image=original,
        target_pose=new_pose,
        prompt="Lady Gaga in blue military costume, arms raised up, dramatic pose, high quality, detailed",
        strength=0.8,
        controlnet_scale=0.9,
        output_dir="/Volumes/Seagate/SAM3/13_SVG_Diffusion/output_pose"
    )


if __name__ == "__main__":
    main()
